# -*- coding: utf-8 -*-
"""genai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1urYi8EfNIvOd5BYejwP6cfUOKNR-7uF3
"""

#1

# Step 1: Uninstall conflicting packages
!pip uninstall -y numpy scipy gensim thinc tsfresh

# Step 2: Install compatible versions
!pip install gensim==4.3.3 numpy==1.26.4 scipy==1.13.1

# Step 3: Run the code
import gensim.downloader as api
import numpy as np

# Load pre-trained GloVe embeddings (100d)
word_vectors = api.load('glove-wiki-gigaword-100')  # ~130MB
print(f"Vocabulary size: {len(word_vectors.index_to_key)}")

# King - Man + Woman = ?
result = word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=5)
for word, similarity in result:
    print(f"{word}: {similarity:.4f}")

# Similar words to "computer"
print(word_vectors.most_similar('computer', topn=5))

# Odd one out
print(word_vectors.doesnt_match("breakfast lunch dinner banana".split()))

# Word similarity
print(word_vectors.similarity('king', 'queen'))

#2

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import numpy as np

# Sports words
sports_words = ['football', 'soccer', 'tennis', 'basketball', 'cricket', 'goal', 'player', 'team', 'coach', 'score']
sports_vectors = np.array([word_vectors[word] for word in sports_words])

# PCA Visualization
pca = PCA(n_components=2)
sports_2d = pca.fit_transform(sports_vectors)
plt.figure(figsize=(8, 6))
for i, word in enumerate(sports_words):
    plt.scatter(sports_2d[i, 0], sports_2d[i, 1])
    plt.annotate(word, (sports_2d[i, 0], sports_2d[i, 1]))
plt.title("PCA Visualization of Sports Words")
plt.show()

# t-SNE Visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=5)
sports_tsne = tsne.fit_transform(sports_vectors)
plt.figure(figsize=(8, 6))
for i, word in enumerate(sports_words):
    plt.scatter(sports_tsne[i, 0], sports_tsne[i, 1])
    plt.annotate(word, (sports_tsne[i, 0], sports_tsne[i, 1]))
plt.title("t-SNE Visualization of Sports Words")
plt.show()

# Function to get 5 semantically similar words
def get_similar_words(word):
    try:
        result = word_vectors.most_similar(word, topn=5)
        for w, sim in result:
            print(f"{w}: {sim:.4f}")
    except KeyError:
        print(f"'{word}' not in vocabulary!")

# Example
get_similar_words('football')

#3

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import numpy as np
import threadpoolctl
import gensim.downloader as api
from gensim.utils import simple_preprocess
from nltk.corpus import stopwords
import nltk

# Download NLTK stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Medical corpus
medical_corpus = [
    "The patient was prescribed antibiotics to treat the bacterial infection.",
    "Diabetes mellitus is characterized by high blood sugar levels.",
    "MRI and CT scans are important tools for diagnosing brain and spinal cord injuries.",
    "Cardiovascular diseases include conditions like heart attack, stroke, and hypertension.",
    "Physical therapy helps patients recover mobility after surgery or injury.",
    "Vaccinations are critical in preventing infectious diseases such as measles and influenza.",
    "Common symptoms of flu include fever, cough, sore throat, and body aches.",
    "Blood pressure monitoring is vital for patients with hypertension or cardiovascular risks.",
    "Surgical removal of tumors requires precise planning and expert care.",
    "Medication dosages must be strictly followed to avoid adverse effects.",
    "Chronic kidney disease often requires dialysis or transplantation.",
    "Asthma is a respiratory condition causing difficulty in breathing due to airway inflammation.",
    "The immune system protects the body against pathogens and foreign substances.",
    "Radiology departments use X-rays, ultrasounds, and MRIs for diagnostic imaging.",
    "Neurological disorders affect the brain, spinal cord, and nerves.",
    "Antiviral drugs are used to treat infections caused by viruses such as HIV and hepatitis.",
    "The doctor ordered blood tests to check for anemia and infection markers.",
    "Diuretics help reduce fluid buildup in patients with heart failure or kidney disease.",
    "Cholesterol levels impact the risk of developing atherosclerosis and heart disease.",
    "Emergency medical services provide urgent care for trauma and critical conditions."
]

# Preprocess function
def preprocess(sentence):
    return [word for word in simple_preprocess(sentence) if word not in stop_words]

# Tokenize corpus
tokenized_corpus = [preprocess(sentence) for sentence in medical_corpus]

# Train Word2Vec model
from gensim.models import Word2Vec
model = Word2Vec(sentences=tokenized_corpus, vector_size=50, window=3, min_count=1, workers=2, epochs=100)

# Find similar words
print("Most similar to 'disease':")
print(model.wv.most_similar('diseases', topn=5))
print("\nMost similar to 'blood':")
print(model.wv.most_similar('blood', topn=5))

#4

word_embeddings = {
    "ai": ["machine learning", "deep learning", "data science"],
    "data": ["information", "dataset", "analytics"],
    "science": ["research", "experiment", "technology"],
    "learning": ["education", "training", "knowledge"],
    "robot": ["automation", "machine", "mechanism"]
}

def find_similar_words(word):
    if word in word_embeddings:
        return word_embeddings[word]
    else:
        return []

def enrich_prompt(prompt):
    words = prompt.lower().split()
    enriched_words = []
    for word in words:
        similar_words = find_similar_words(word)
        if similar_words:
            enriched_words.append(f"{word} ({', '.join(similar_words)})")
        else:
            enriched_words.append(word)
    return " ".join(enriched_words)

original_prompt = "Explain AI and its applications in science."
enriched_prompt = enrich_prompt(original_prompt)

print("Original Prompt:")
print(original_prompt)
print("\nEnriched Prompt:")
print(enriched_prompt)

#5

word_embeddings = {
    "adventure": ["journey", "exploration", "quest"],
    "robot": ["machine", "automation", "mechanism"],
    "forest": ["woods", "jungle", "wilderness"],
    "ocean": ["sea", "waves", "depths"],
    "magic": ["spell", "wizardry", "enchantment"]
}

def get_similar_words(seed_word):
    if seed_word in word_embeddings:
        return word_embeddings[seed_word]
    else:
        return ["No similar words found"]

def create_paragraph(seed_word):
    similar_words = get_similar_words(seed_word)
    if "No similar words found" in similar_words:
        return f"Sorry, I couldn't find similar words for '{seed_word}'."
    paragraph = (
        f"Once upon a time, there was a great {seed_word}. "
        f"It was full of {', '.join(similar_words[:-1])}, and {similar_words[-1]}. "
        f"Everyone who experienced this {seed_word} always remembered it as a remarkable tale."
    )
    return paragraph

seed_word = "adventure"  # Change to "robot", "forest", "ocean", "magic", etc.

story = create_paragraph(seed_word)
print("Generated Paragraph:")
print(story)

#6

from transformers import pipeline
# Load the sentiment-analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")
# Example feedback sentences (real-world inputs)
reviews = [
    "The product quality is amazing!",
    "I received the item.",
    "Customer service was terrible.",
    "I'm extremely satisfied with the delivery speed.",
    "The item broke after two days, very disappointed.",
    "Great value for money. Will buy again!"
]
# Analyze sentiment for each review
for review in reviews:
    result = sentiment_pipeline(review)[0] # returns a list with one dict
print(f"Review: \"{review}\"\n â†’ Sentiment: {result['label']}, Score: {result['score']:.4f}\n")

#7

from transformers import pipeline

summarizer = pipeline("summarization")

long_text = """
Artificial Intelligence (AI) is transforming various industries by automating tasks, improving
efficiency,
and enabling new capabilities. In the healthcare sector, AI is used for disease diagnosis,
personalized medicine,
and drug discovery. In the business world, AI-powered systems are optimizing customer
service, fraud detection,
and supply chain management. AI's impact on everyday life is significant, from smart
assistants to recommendation
systems in streaming platforms. As AI continues to evolve, it promises even greater
advancements in fields like
education, transportation, and environmental sustainability.
"""

summary = summarizer(long_text, max_length=50, min_length=20, do_sample=False)[0]["summary_text"]

print("Summarized Text:")
print(summary)

#8
#keep a crow.txt file uploaded in your drive and add the path

!pip install langchain cohere langchain-community

from langchain.llms import Cohere
from langchain.prompts import PromptTemplate
from langchain import LLMChain
from google.colab import drive

drive.mount('/content/drive')

file_path = "/content/drive/MyDrive/Text/crow.txt"
with open(file_path, "r") as file:
    text = file.read()

cohere_api_key = "xNBCmAElkHo9M2tjr6d3SLP80NbMt8MofoUMsREE"

prompt_template = """
Summarize the following text in two bullet points:
{text}
"""

llm = Cohere(cohere_api_key=cohere_api_key)
prompt = PromptTemplate(input_variables=["text"], template=prompt_template)

chain = LLMChain(llm=llm, prompt=prompt)

result = chain.run(text)

print(text)
print("Summarized Output in Bullet Points:")
print(result)

#9

!pip install --quiet langchain pydantic wikipedia-api langchain-core cohere langchain-community

from langchain_community.llms import Cohere
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from pydantic import BaseModel
import wikipediaapi

class InstitutionDetails(BaseModel):
    founder: str
    founded: str
    branches: str
    employees: str
    summary: str

def fetch_wikipedia_summary(institution_name, max_chars=3000):
    wiki = wikipediaapi.Wikipedia(language='en', user_agent='InstitutionInfoBot/1.0 (https://www.wikipedia.org/)')
    page = wiki.page(institution_name)
    if not page.exists():
        return "No information available."
    return page.text[:max_chars]

prompt_template = """
Extract the following information from the given text:
- Founder
- Founded (year)
- Current branches
- Number of employees
- 4-line brief summary

Text: {text}

Format:
Founder: <founder>
Founded: <founded>
Branches: <branches>
Employees: <employees>
Summary: <summary>
"""

if __name__ == "__main__":
    institution_name = input("Enter the name of the institution: ")
    wiki_text = fetch_wikipedia_summary(institution_name)

    # Replace with your real key, keep it secret in production
    llm = Cohere(cohere_api_key="cHb51fJ9urTonXJaiG6l7sOGUoMseDK6tMGz8mRN")

    prompt = PromptTemplate.from_template(prompt_template)

    chain = prompt | llm  # pipe prompt into llm

    response = chain.invoke({"text": wiki_text})

    try:
        lines = response.strip().split('\n')
        info = {line.split(':')[0].lower(): ':'.join(line.split(':')[1:]).strip() for line in lines if ':' in line}
        details = InstitutionDetails(
            founder=info.get("founder", "N/A"),
            founded=info.get("founded", "N/A"),
            branches=info.get("branches", "N/A"),
            employees=info.get("employees", "N/A"),
            summary=info.get("summary", "N/A")
        )
        print("\nInstitution Details:")
        print(f"Founder: {details.founder}")
        print(f"Founded: {details.founded}")
        print(f"Branches: {details.branches}")
        print(f"Employees: {details.employees}")
        print(f"Summary: {details.summary}")
    except Exception as e:
        print("Error parsing response:", e)

#10
#create a file named "ipc_file.txt" and paste the path in ipc_file_path=

!pip install langchain pydantic wikipedia-api cohere langchain-community

from langchain.chains.question_answering import load_qa_chain
from langchain.docstore.document import Document
from langchain_community.llms import Cohere

ipc_file_path = "/content/ipc_file.txt"
with open(ipc_file_path, "r", encoding="utf-8") as file:
    ipc_text = file.read()

ipc_document = Document(page_content=ipc_text)

cohere_api_key = "cHb51fJ9urTonXJaiG6l7sOGUoMseDK6tMGz8mRN"
llm = Cohere(cohere_api_key=cohere_api_key, temperature=0.3)

qa_chain = load_qa_chain(llm, chain_type="stuff")

print("Chatbot for the Indian Penal Code (IPC)")
print("Ask a question about the Indian Penal Code (type 'exit' to stop):")

while True:
    user_question = input("\nYour question: ")
    if user_question.lower() == "exit":
        print("Goodbye!")
        break
    response = qa_chain.run(input_documents=[ipc_document], question=user_question)
    print(f"Answer: {response}")